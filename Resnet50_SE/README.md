# Squeeze and Excitation

The [Squeeze and Excitation](https://arxiv.org/abs/1709.01507) Attention Module aims to highligth the channel features by using average pooling on them.
This module can be inserted between convolutional layers since it goes from a feature map and solves the highlighed features on it.

<img src="https://miro.medium.com/max/1120/1*bmObF5Tibc58iE9iOu327w.png">

## Results
